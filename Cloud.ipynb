{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrvZ8m1jpPfQ"
      },
      "outputs": [],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKx6oszTpkbW"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"Swiggy\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/drive/MyDrive/Swiggy.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Display the schema and first 5 rows of the DataFrame\n",
        "df.printSchema()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpvbeux3pkmz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"ColumnRemovalExample\").getOrCreate()\n",
        "\n",
        "# Assuming df is your PySpark DataFrame\n",
        "df_filtered = df.filter((col(\"rating\").cast(\"string\") != \"--\") & (col(\"cost\") != \"₹\"))\n",
        "\n",
        "# List of columns to be removed\n",
        "columns_to_remove = [\"licension no\", \"restaurant link\", \"menu\", \"city link\", \"subcity\", \"subcity link\", \"restaurant code\", \"price\"]\n",
        "\n",
        "# Remove specified columns\n",
        "df_filtered = df_filtered.drop(*columns_to_remove)\n",
        "\n",
        "# Replace \"₹\" with \"Rs\" in the \"cost\" column\n",
        "df_filtered = df_filtered.withColumn(\"cost\", when(col(\"cost\") == \"₹\", \"Rs\").otherwise(col(\"cost\")))\n",
        "\n",
        "# Show the updated DataFrame\n",
        "df_filtered.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuJYCsV49qZX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "# Count null values in each column of the updated DataFrame\n",
        "null_counts = df_filtered.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_filtered.columns]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WH_YGrL9Ypn"
      },
      "outputs": [],
      "source": [
        "# Drop rows with null values\n",
        "df_no_nulls = df_filtered.na.drop()\n",
        "\n",
        "# Show the updated DataFrame with no nulls\n",
        "df_no_nulls.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFMBIsgXumFr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "# Count null values in each column of the updated DataFrame\n",
        "null_counts = df_no_nulls.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_no_nulls.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZbKOUDrygfc"
      },
      "outputs": [],
      "source": [
        "# Specify the output path for the CSV file\n",
        "output_path = \"/content/drive/MyDrive/CA640\"\n",
        "\n",
        "# Reduce the number of partitions to 1\n",
        "df_single_partition = df_no_nulls.coalesce(1)\n",
        "\n",
        "# Write the DataFrame to a single CSV file\n",
        "df_single_partition.write.csv(output_path, header=True, mode=\"overwrite\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_csv_file.csv' with the actual file path\n",
        "csv_file_path = '/content/drive/MyDrive/CA640/part-00000-a7e7c7f2-61bd-4889-97f3-d589dfa81b67-c000.csv'\n",
        "\n",
        "# Read data from CSV into a DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "GHrpnUVTS73w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YasSmHcUTEO9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}